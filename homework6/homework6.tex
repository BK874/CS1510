
\documentclass[letterpaper,notitlepage,twoside]{article}

% Basic imports, increase margins...
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

% Finite State Machine stuff
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage{pgfplots}

% Format tables nicely
\usepackage[latin1]{inputenc}
\usepackage{array}
\usepackage{booktabs}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}

\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{amsmath,amsthm}

\renewcommand{\implies}{\Rightarrow} % redefine command "implies"  
\renewcommand{\iff}{\Leftrightarrow} % double arrow
\newcommand{\maps}{\rightarrow} % define command "map" 
\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\newcommand{\N}{\mathbb{N}} % natural number 
\newcommand{\Q}{\mathbb{Q}} % rational number 
\newcommand{\R}{\mathbb{R}} % real number 
\newcommand{\Z}{\mathbb{Z}} % integers 
\newcommand\tab[1][1cm]{\hspace*{#1}} %\tab command

% Add more packages that you use here..

\title{CS 1510 Homework 6}
\author{Brian Falkenstein, Brian Knotten, Brett Schreiber}

\begin{document}
\maketitle

\section*{8}
Define algorithm $A$ : 
\begin{enumerate}
\item Read the next page access in the input sequence.
\item If fast memory is not full, add the page into fast memory. 
\item If fast memory is full, evict the page that will be accessed furthest into the future by scanning through the input sequence. If a page in fast memory does not get accessed again, evict that page.
\item Repeat for all pages in the input sequence.
\end{enumerate} 
This algorithm is correct for all inputs. Assume to reach a contradiction that $A$ is not correct, and there exists some input $I$ that causes $A$'s output to be suboptimal. Define $Opt(I)$ to be the optimal output on $I$ that agrees with $A(I)$ for the most steps (where a step is a one iteration in the algorithm, either evicting or not evicting a page). Let $t$ be the first time that $A(I)$ and $Opt(I)$ disagree. At $t$, the two algorithms disagree  because $A(I)$ chose to evict a page $x$ and $Opt(I)$ evicted page $w$, to swap in page $z$. It couldn't be the case that $A(I)$ chose to evict a page and $Opt(I)$ was able to add the page to fast memory without evicting, because this would contradict our definition of $A$. It also couldn't be the case that $Opt(I)$ evicted a page when $A(I)$ didn't have to, because the algorithms agreed up until this point, and there is no way this could be an optimal solution, as you could decrease the amount of evictions necessary by simply adding $z$ into fast memory without any evictions. \\
We can construct $Opt'(I)$ by instead of evicting $w$ at time $t$, evict $x$, to agree with $A$ at $t$. This clearly agrees with $A(I)$ for one more step. This solution is also still optimal. If neither $w$ or $x$ occurs again in the input sequence, then this decision makes no difference. If its the case that $w$ occurs again, and $x$ does not, then $A(I)$ is more optimal than $Opt(I)$, as $Opt$ will need to make another eviction to add $w$ into fast memory, and $A$ will not. If its the case that both $w$ and $x$ occur again, then $x$ will occur after $w$, because $A$ evicted $x$, and by definition $A$ will evict the page that will need to be accessed furthest into the future. Then, when $w$ occurs again, it will still be in $A(I)'s$ fast memory and $A$ will not require an eviction, whereas $Opt$'s solution did. Thus, we have constructed an output that agrees with $A$ for one more step while still being an optimal solution, a contradiction. 

\section*{1}
\subsection*{a}
Let $Time$ be a function denoting the number of steps required to compute a $T$ on an input. \\
Computing $T(n)$ recursively requires at least the number of steps to calculate $T(n - 1)$ and $T(n - 2)$, as well as a multiplication step. \\
$Time(n) \geq Time(n - 1) + Time(n - 2) + 1$ \\
So computing $T(n)$ takes more steps than computing $T(n - 2)$ twice. \\
$Time(n) \geq 2 * Time(n - 2)$ \\
$Time(n) \geq 4 * Time(n - 4) \geq 8 * Time(n - 6) \geq 16 * Time(n - 8) ...$ \\
$Time(n) \geq 2^{n/2}$ \\
So computing $T(n)$ requires exponentially many operations.

\subsection*{b}
Assuming $T(0), T(1), T(2)...T(n -2), T(n - 1)$ are already computed, it takes $n$ multiplication steps, $T(0) * T(1), T(1) * T(2)...T(n - 2) * T(n - 1)$ and $n$ addition steps to compute $T(n)$, so it takes $2n = O(n)$ steps to do the multiplication and addition. \\
From the bottom up, $T(0), T(1), T(2)...T(n - 1)$ need to be computed, so that is $n$ computations, each of which take $O(n)$ steps. Therefore by starting from the bottom up and caching results, computing $T(n)$ takes $O(n^2)$ steps.

\subsection*{c}
\begin{verbatim}
int T(int n) {
    int T[n + 1];
    T[0] = 2;
    T[1] = 2;
    
    int sum = 0;
    for(int i = 2; i <= n; i++) {
        sum += T[i - 1] * T[i - 2];
        T[i] = sum;
    }
    
    return T[n];
}
\end{verbatim}

\end{document}
